# ğŸ—ï¸ Wiber Architecture

## System Overview

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      User/Client        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                     HTTP POST /messages
                     HTTP GET /messages
                                â”‚
                                â†“
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘         REST API (Port 8000)              â•‘
        â•‘  - FastAPI                                â•‘
        â•‘  - Message validation                     â•‘
        â•‘  - Health checks                          â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                        â”‚
     Kafka Producer              MongoDB Query
            â”‚                        â”‚
            â†“                        â†“
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     Kafka     â•‘        â•‘   MongoDB     â•‘
    â•‘  (Port 9092)  â•‘        â•‘ (Port 27017)  â•‘
    â•‘               â•‘        â•‘               â•‘
    â•‘ Topic:        â•‘        â•‘ Database:     â•‘
    â•‘ wiber.messagesâ•‘        â•‘ wiber         â•‘
    â•‘               â•‘        â•‘               â•‘
    â•‘ Partitions: 3 â•‘        â•‘ Collection:   â•‘
    â•‘               â•‘        â•‘ messages      â•‘
    â•šâ•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            â”‚                        â†‘
            â”‚                        â”‚
       Kafka Consumer                â”‚
            â”‚                        â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   Save Message
```

## Component Details

### 1. REST API (Port 8000)
**Technology**: FastAPI + Uvicorn
**Responsibilities**:
- Accept HTTP requests from users
- Validate message format
- Produce messages to Kafka
- Query messages from MongoDB
- Provide health checks

**Key Endpoints**:
- `POST /messages` - Send a message
- `GET /messages/{user1}/{user2}` - Get conversation
- `GET /health/readiness` - Check if ready

### 2. Kafka (Port 9092)
**Technology**: Apache Kafka (KRaft mode - no Zookeeper)
**Responsibilities**:
- Queue messages reliably
- Distribute messages across partitions
- Maintain message ordering per partition
- Replicate data for fault tolerance

**Configuration**:
- **Topic**: `wiber.messages`
- **Partitions**: 3 (configurable)
- **Replication Factor**: 1 (dev), 3 (production)
- **Retention**: 7 days (default)

### 3. Consumer Service
**Technology**: Python + kafka-python
**Responsibilities**:
- Subscribe to Kafka topics
- Process messages one by one
- Save to MongoDB
- Commit offset after successful save

**Fault Tolerance**:
- Manual commit (at-least-once delivery)
- Graceful shutdown
- Automatic rebalancing

### 4. MongoDB (Port 27017)
**Technology**: MongoDB 6.0
**Responsibilities**:
- Store messages permanently
- Provide fast queries
- Ensure no duplicates (unique index)
- Maintain data consistency

**Indexes**:
- `messageId` (unique)
- `(fromUser, toUser, timestamp)` (compound)
- `timestamp` (for sorting)

### 5. AKHQ (Port 8080)
**Technology**: Kafka Web UI
**Responsibilities**:
- Monitor Kafka topics
- View messages in real-time
- Inspect consumer groups
- Debug issues

---

## Message Flow

### Sending a Message

```
1. User sends POST request
   â†“
2. API validates request
   â†“
3. API generates messageId & timestamp
   â†“
4. Producer sends to Kafka
   â”œâ”€ Key: "alice:bob" (for ordering)
   â”œâ”€ Value: {messageId, fromUser, toUser, content, timestamp}
   â””â”€ Topic: "wiber.messages"
   â†“
5. Kafka stores in partition (based on key hash)
   â†“
6. API returns response immediately
   (Message processing continues asynchronously)
```

### Receiving a Message

```
1. Consumer polls Kafka
   â†“
2. Kafka returns next message
   â†“
3. Consumer deserializes JSON
   â†“
4. Consumer saves to MongoDB
   â”œâ”€ Unique index prevents duplicates
   â””â”€ Write concern ensures durability
   â†“
5. Consumer commits offset
   (Tells Kafka: "I processed this message")
```

### Querying Messages

```
1. User sends GET request
   â†“
2. API queries MongoDB
   â”œâ”€ Filter: fromUser/toUser match
   â”œâ”€ Sort: timestamp ascending
   â””â”€ Limit: 50 messages
   â†“
3. MongoDB uses index for fast lookup
   â†“
4. API returns results as JSON
```

---

## Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   alice     â”‚  POST /messages
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  {from: alice, to: bob, content: "Hi"}
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API: Create Message                        â”‚
â”‚  - messageId: "123e4567..."                 â”‚
â”‚  - timestamp: 1696176000000                 â”‚
â”‚  - key: "alice:bob"                         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“  Kafka.send(topic="wiber.messages", key="alice:bob", value={...})
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka: Store in Partition                  â”‚
â”‚                                             â”‚
â”‚  Partition 0: [msg from alice:charlie]      â”‚
â”‚  Partition 1: [msg from alice:bob] â† HERE   â”‚
â”‚  Partition 2: [msg from bob:charlie]        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“  Consumer.poll()
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Consumer: Process Message                  â”‚
â”‚  1. Deserialize JSON                        â”‚
â”‚  2. Validate fields                         â”‚
â”‚  3. Save to MongoDB                         â”‚
â”‚  4. Commit offset                           â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“  db.messages.insert_one({...})
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MongoDB: Store Document                    â”‚
â”‚                                             â”‚
â”‚  {                                          â”‚
â”‚    messageId: "123e4567...",                â”‚
â”‚    fromUser: "alice",                       â”‚
â”‚    toUser: "bob",                           â”‚
â”‚    content: "Hi",                           â”‚
â”‚    timestamp: 1696176000000                 â”‚
â”‚  }                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†‘  GET /messages/alice/bob
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API: Query Messages                        â”‚
â”‚  db.messages.find({                         â”‚
â”‚    $or: [                                   â”‚
â”‚      {fromUser: "alice", toUser: "bob"},    â”‚
â”‚      {fromUser: "bob", toUser: "alice"}     â”‚
â”‚    ]                                        â”‚
â”‚  }).sort({timestamp: 1})                    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“  Return JSON array
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     bob     â”‚  [{"messageId": "123...", "content": "Hi", ...}]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Fault Tolerance Mechanisms

### 1. Producer Fault Tolerance
```
Producer Config:
â”œâ”€ acks='all'           â†’ Wait for all replicas
â”œâ”€ retries=10           â†’ Retry on failure
â””â”€ enable_idempotence   â†’ No duplicates in Kafka
```

### 2. Consumer Fault Tolerance
```
Consumer Config:
â”œâ”€ enable_auto_commit=False  â†’ Manual commit
â”œâ”€ Save to DB                â†’ At-least-once
â””â”€ Commit offset             â†’ Only after success
```

### 3. Database Fault Tolerance
```
MongoDB Config:
â”œâ”€ WriteConcern(w='majority')  â†’ Majority replicas confirm
â”œâ”€ j=True                      â†’ Write to journal (disk)
â””â”€ Unique index on messageId   â†’ No duplicates
```

### 4. System Fault Tolerance
```
Docker Config:
â”œâ”€ restart: unless-stopped     â†’ Auto-restart on crash
â”œâ”€ healthcheck                 â†’ Monitor health
â””â”€ depends_on                  â†’ Start in correct order
```

---

## Scaling Strategy

### Horizontal Scaling (Add More Instances)

```
# Scale consumers
docker compose up -d --scale consumer=3

Result:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Topic: wiber.messages (3 partitions)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Partition 0 â†’ Consumer 1           â”‚
â”‚  Partition 1 â†’ Consumer 2           â”‚
â”‚  Partition 2 â†’ Consumer 3           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Each consumer processes its own partition independently!
```

### Vertical Scaling (More Resources)

```yaml
# In docker-compose.yml
consumer:
  deploy:
    resources:
      limits:
        cpus: '2.0'
        memory: 2G
```

---

## Monitoring Points

### 1. API Metrics
- Request rate (requests/sec)
- Response time (latency)
- Error rate (%)
- Active connections

### 2. Kafka Metrics
- Messages produced/sec
- Messages consumed/sec
- Consumer lag (messages behind)
- Partition distribution

### 3. MongoDB Metrics
- Write throughput (writes/sec)
- Read throughput (reads/sec)
- Index hit ratio (%)
- Storage size

### 4. System Metrics
- CPU usage (%)
- Memory usage (%)
- Disk I/O (MB/sec)
- Network traffic (MB/sec)

---

## Security Considerations (Future Work)

### Authentication & Authorization
- Add JWT tokens for API authentication
- Implement user permissions
- Encrypt messages in transit (TLS)

### Network Security
- Use Docker networks to isolate services
- Firewall rules for production
- VPN for remote access

### Data Security
- Encrypt messages at rest in MongoDB
- Rotate credentials regularly
- Audit logs for compliance

---

## Deployment Environments

### Development (Current Setup)
```
- Single machine
- All services in Docker
- No replication (Kafka RF=1, MongoDB single node)
- Local ports exposed
```

### Production (Recommended)
```
- Kubernetes cluster
- Kafka with 3+ brokers (RF=3)
- MongoDB replica set (3+ nodes)
- Load balancer for API
- Prometheus + Grafana monitoring
- Separate networks for security
```

---

This architecture provides:
- âœ… **Fault Tolerance**: Automatic retries, replication, health checks
- âœ… **Consistency**: Unique indexes, ordered reads, write concerns
- âœ… **Scalability**: Horizontal scaling via partitions and consumer groups
- âœ… **Performance**: Async processing, indexed queries, efficient serialization
- âœ… **Observability**: Health endpoints, logs, monitoring UI


